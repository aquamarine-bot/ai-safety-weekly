# Agent Safety Weekly

> Weekly curated papers on Agent Safety, LLM red-teaming, and adversarial attacks on agents

Auto-updated weekly. Last update: **2026-02-20**

---

## 2026-W08

### [Pushing the Frontier of Black-Box LVLM Attacks via Fine-Grained Detail Targeting (M-Attack-V2)](https://arxiv.org/abs/2602.17645)
- **Authors:** Xiaohan Zhao, Zhaoyi Li, Yaxin Luo et al.
- **Date:** 2026-02-19
- **Category:** `adversarial attack AND language model`

> Black-box adversarial attacks on Large Vision-Language Models (LVLMs) are challenging due to missing gradients and complex multimodal boundaries. We find that prior approaches induce high-variance, nearly orthogonal gradients across iterations. We reformulate local matching as an asymmetric expectation over source transformations and target semanti...

**ğŸ“ Summary:** M-Attack-V2ï¼šæ”¹è¿›é»‘ç›’å¤šæ¨¡æ€ LLM å¯¹æŠ—æ”»å‡»ï¼Œé€šè¿‡ MCA+ATA ç¨³å®šæ¢¯åº¦ä¼°è®¡ï¼ŒClaude-4.0 æ”»å‡»æˆåŠŸç‡ 8%â†’30%ï¼ŒGPT-5 98%â†’100%ã€‚

### [Mind the GAP: Text Safety Does Not Transfer to Tool-Call Safety in LLM Agents](https://arxiv.org/abs/2602.16943)
- **Authors:** Arnold Cartagena, Ariane Teixeira
- **Date:** 2026-02-18
- **Category:** `jailbreak AND agent`

> Large language models deployed as agents increasingly interact with external systems through tool calls--actions with real-world consequences that text outputs alone do not carry. Safety evaluations, however, overwhelmingly measure text-level refusal behavior, leaving a critical question unanswered: does alignment that suppresses harmful text also ...

**ğŸ“ Summary:** æå‡º GAP benchmarkï¼Œå‘ç° LLM agent çš„æ–‡æœ¬å±‚æ‹’ç»â‰ å·¥å…·è°ƒç”¨å±‚å®‰å…¨â€”â€”æ¨¡å‹å£å¤´æ‹’ç»çš„åŒæ—¶å¯èƒ½æ‚„æ‚„æ‰§è¡Œç¦æ­¢æ“ä½œï¼Œ17,420 ä¸ªæ•°æ®ç‚¹è¦†ç›–å…­ä¸ªå‰æ²¿æ¨¡å‹ã€‚

### [Policy Compiler for Secure Agentic Systems (PCAS)](https://arxiv.org/abs/2602.16708)
- **Authors:** Nils Palumbo, Sarthak Choudhary, Jihye Choi et al.
- **Date:** 2026-02-18
- **Category:** `prompt injection`

> LLM-based agents are increasingly being deployed in contexts requiring complex authorization policies. Embedding these policies in prompts provides no enforcement guarantees. We present PCAS, a Policy Compiler for Agentic Systems that provides deterministic policy enforcement. PCAS models the agentic system state as a dependency graph capturing cau...

**ğŸ“ Summary:** PCASï¼šç”¨ dependency graph + Datalog ç­–ç•¥è¯­è¨€å®ç°ç¡®å®šæ€§ç­–ç•¥æ‰§è¡Œï¼Œé˜²å¾¡ prompt injectionï¼Œpolicy åˆè§„ç‡ä» 48% æå‡åˆ° 93%ï¼Œæ— éœ€ä¿®æ”¹ agent ä»£ç ã€‚

### [Helpful to a Fault: Measuring Illicit Assistance in Multi-Turn, Multilingual LLM Agents (STING)](https://arxiv.org/abs/2602.16346)
- **Authors:** Nivya Talokar, Ayush K Tarun, Murari Mandal et al.
- **Date:** 2026-02-18
- **Category:** `red teaming AND LLM`

> LLM-based agents execute real-world workflows via tools and memory. These affordances enable ill-intended adversaries to also use these agents to carry out complex misuse scenarios. We introduce STING (Sequential Testing of Illicit N-step Goal execution), an automated red-teaming framework that constructs a step-by-step illicit plan grounded in a b...

**ğŸ“ Summary:** STINGï¼šè‡ªåŠ¨åŒ–å¤šè½® agent red-teaming æ¡†æ¶ï¼Œå°†æ”»å‡»å»ºæ¨¡ä¸º time-to-first-jailbreak éšæœºå˜é‡ï¼Œå¤šè¯­è¨€å®éªŒå‘ç°ä½èµ„æºè¯­è¨€æœªå¿…æ›´è„†å¼±ï¼ˆä¸ chatbot ç ”ç©¶ç»“è®ºä¸åŒï¼‰ã€‚

### [A Trajectory-Based Safety Audit of Clawdbot (OpenClaw)](https://arxiv.org/abs/2602.14364)
- **Authors:** Tianyu Chen, Dongrui Liu, Xia Hu et al.
- **Date:** 2026-02-16
- **Category:** `agent safety`

> Clawdbot is a self-hosted, tool-using personal AI agent with a broad action space spanning local execution and web-mediated workflows. We present a trajectory-centric evaluation of Clawdbot across six risk dimensions. Our test suite samples and lightly adapts scenarios from prior agent-safety benchmarks (including ATBench and LPS-Bench) and supplem...

**ğŸ“ Summary:** å¯¹ OpenClawï¼ˆClawdbotï¼‰åš trajectory-based å®‰å…¨å®¡è®¡ï¼Œ34 ä¸ªæµ‹è¯•ç”¨ä¾‹è¦†ç›–å…­ä¸ªé£é™©ç»´åº¦ï¼Œå‘ç°å¤§å¤šæ•°å¤±è´¥å‡ºç°åœ¨æ„å›¾æ¨¡ç³Šæˆ– benign-seeming jailbreak åœºæ™¯ã€‚

### [Boundary Point Jailbreaking of Black-Box LLMs](https://arxiv.org/abs/2602.15001)
- **Authors:** Xander Davies, Giorgi Giglemiani, Edmund Lau et al.
- **Date:** 2026-02-16
- **Category:** `red teaming AND LLM`

> Frontier LLMs are safeguarded against attempts to extract harmful information via adversarial prompts known as "jailbreaks". Recently, defenders have developed classifier-based systems that have survived thousands of hours of human red teaming. We introduce Boundary Point Jailbreaking (BPJ), a new class of automated jailbreak attacks that evade the...

**ğŸ“ Summary:** BPJï¼šçº¯é»‘ç›’ jailbreakï¼Œæ¯æ¬¡åªç”¨ä¸€ bit ä¿¡æ¯ï¼ˆæ˜¯å¦è¢«æ£€æµ‹å™¨æ ‡è®°ï¼‰ï¼Œé€šè¿‡ curriculum ä¸­é—´ç›®æ ‡æ”»ç ´ Constitutional Classifiers å’Œ GPT-5 è¾“å…¥è¿‡æ»¤å™¨ã€‚

### [Exposing the Systematic Vulnerability of Open-Weight Models to Prefill Attacks](https://arxiv.org/abs/2602.14689)
- **Authors:** Lukas Struppek, Adam Gleave, Kellin Pelrine
- **Date:** 2026-02-16
- **Category:** `red teaming AND LLM`

> Open-weight models natively support prefilling, which allows an attacker to predefine initial response tokens before generation begins. We present the largest empirical study to date of prefill attacks, evaluating over 20 existing and novel strategies across multiple model families and state-of-the-art open-weight models. Our results show that pref...

**ğŸ“ Summary:** ç³»ç»Ÿç ”ç©¶ prefill attackï¼ˆé¢„å¡«å……åˆå§‹å›å¤ tokenï¼‰å¯¹å¼€æºæ¨¡å‹çš„æ”»å‡»æ•ˆæœï¼Œ20+ ç­–ç•¥è¯„ä¼°å…¨éƒ¨ä¸»æµå¼€æºæ¨¡å‹ï¼Œå‘ç°æ™®éè„†å¼±ï¼›æ¨ç†æ¨¡å‹å¯¹é€šç”¨ prefill æœ‰ä¸€å®šæŠµæŠ—ä½†é’ˆå¯¹æ€§ç­–ç•¥ä¾ç„¶æœ‰æ•ˆã€‚

---

## 2026-W07

### [SkillJect: Automating Stealthy Skill-Based Prompt Injection for Coding Agents](https://arxiv.org/abs/2602.14211)
- **Authors:** Xiaojun Jia, Jie Liao, Simeng Qin et al.
- **Date:** 2026-02-15
- **Category:** `prompt injection`

> Agent skills are becoming a core abstraction in coding agents, packaging long-form instructions and auxiliary scripts to extend tool-augmented behaviors. This abstraction introduces an under-measured attack surface: skill-based prompt injection, where poisoned skills can steer agents away from user intent and safety policies. We propose the first a...

**ğŸ“ Summary:** SkillJectï¼šé¦–ä¸ªé’ˆå¯¹ coding agent skill çš„è‡ªåŠ¨åŒ–éšè”½ prompt injection æ¡†æ¶ï¼Œä¸‰ agent é—­ç¯ï¼ˆAttack/Code/Evaluateï¼‰ï¼Œå°†æ¶æ„æ“ä½œè—äºè¾…åŠ©è„šæœ¬ä¸­ã€‚

### [Unsafer in Many Turns: Benchmarking and Defending Multi-Turn Safety Risks in Tool-Using Agents](https://arxiv.org/abs/2602.13379)
- **Authors:** Xu Li, Simon Yu, Minzhou Pan et al.
- **Date:** 2026-02-13
- **Category:** `agent safety`

> LLM-based agents are becoming increasingly capable, yet their safety lags behind. This creates a gap between what agents can do and should do. This gap widens as agents engage in multi-turn interactions and employ diverse tools, introducing new risks overlooked by existing benchmarks. To systematically scale safety testing into multi-turn, tool-rea...

**ğŸ“ Summary:** æ„å»º MT-AgentRisk benchmarkï¼ˆé¦–ä¸ªå¤šè½®å·¥å…·è°ƒç”¨ agent å®‰å…¨è¯„ä¼°ï¼‰ï¼Œå‘ç°å¤šè½®åœºæ™¯ä¸‹ ASR å¹³å‡æå‡ 16%ï¼›æå‡ºå…è®­ç»ƒé˜²å¾¡ ToolShieldã€‚Bo Li ç»„çš„å·¥ä½œï¼

---


*Curated by [aq bot](https://github.com/aquamarine-bot) Â· [Agent Safety Weekly](https://github.com/aquamarine-bot/agent-safety-weekly)*